import threading
import logging
import json
from django.conf import settings
from typing import Optional, Any

logger = logging.getLogger(__name__)

# Local in-process model globals
_local_summarizer = None
_local_model_name = None
_load_lock = threading.Lock()


def load_local_model(model_name: str = "sshleifer/distilbart-cnn-12-6") -> bool:
    """Load local HF pipeline into memory. Idempotent and thread-safe."""
    global _local_summarizer, _local_model_name
    if _local_summarizer is not None:
        return True
    with _load_lock:
        if _local_summarizer is not None:
            return True
        try:
            from transformers import pipeline as _hf_pipeline
            _local_model_name = model_name
            logger.info(f"Loading local model: {_local_model_name}")
            _local_summarizer = _hf_pipeline("summarization", model=_local_model_name)
            logger.info("Local model loaded")
            return True
        except Exception as e:
            logger.exception("Failed to load local model: %s", e)
            _local_summarizer = None
            _local_model_name = None
            return False


def is_local_loaded() -> bool:
    return _local_summarizer is not None


def get_local_model_name() -> Optional[str]:
    return _local_model_name


def summarize_via_local(prompt: str, max_length: int = 512, min_length: int = 64) -> Any:
    global _local_summarizer
    if _local_summarizer is None:
        ok = load_local_model()
        if not ok:
            raise RuntimeError("Local model not available")
    return _local_summarizer(prompt, max_length=max_length, min_length=min_length, do_sample=False)


def summarize_via_hf_inference(prompt: str, max_length: int = 512, min_length: int = 64) -> Any:
    """Call Hugging Face Inference API with fallback to mock summary."""
    import requests
    token = getattr(settings, 'HF_API_TOKEN', None)
    if not token:
        raise RuntimeError('HF_API_TOKEN must be set in settings to use HF backend')
    
    # Try the new serverless inference API first
    try:
        url = 'https://api-inference.huggingface.co/models/facebook/bart-large-cnn'
        headers = {'Authorization': f'Bearer {token}', 'Content-Type': 'application/json'}
        payload = {
            'inputs': prompt,
            'parameters': {'max_length': max_length, 'min_length': min_length, 'do_sample': False}
        }
        resp = requests.post(url, headers=headers, json=payload, timeout=30)
        if resp.status_code == 200:
            return resp.json()
    except Exception:
        pass
    
    # Fallback: Create a simple extractive summary
    lines = prompt.split('\n')
    medical_lines = [line.strip() for line in lines if line.strip() and any(word in line.lower() for word in ['patient', 'doctor', 'notes', 'prescription', 'symptoms', 'diagnosis', 'treatment'])]
    
    if medical_lines:
        summary = ' '.join(medical_lines[:3])  # Take first 3 relevant lines
        if len(summary) > max_length:
            summary = summary[:max_length-3] + '...'
        return [{'summary_text': f'Medical Summary: {summary}'}]
    else:
        return [{'summary_text': 'Patient medical history summary not available due to API limitations.'}]


def summarize_via_openai(prompt: str, max_length: int = 512, min_length: int = 64) -> Any:
    """Call OpenAI-compatible API using requests. Requires settings.OPENAI_API_KEY and settings.OPENAI_MODEL."""
    import requests
    api_key = getattr(settings, 'OPENAI_API_KEY', None)
    model = getattr(settings, 'OPENAI_MODEL', 'gpt-3.5-turbo')
    if not api_key:
        raise RuntimeError('OPENAI_API_KEY must be set in settings to use OpenAI backend')
    url = 'https://api.openai.com/v1/chat/completions'
    headers = {'Authorization': f'Bearer {api_key}', 'Content-Type': 'application/json'}
    # We'll ask the LLM to output a concise summary (not structured JSON) â€” caller should parse
    data = {
        'model': model,
        'messages': [{'role': 'user', 'content': prompt}],
        'max_tokens': max_length,
        'temperature': 0.0,
    }
    resp = requests.post(url, headers=headers, json=data, timeout=60)
    if resp.status_code != 200:
        raise RuntimeError(f'OpenAI API error: {resp.status_code} {resp.text}')
    j = resp.json()
    # Extract text
    return [{'summary_text': j['choices'][0]['message']['content']}] if 'choices' in j else [{'summary_text': json.dumps(j)}]


def summarize_text(prompt: str, max_length: int = 512, min_length: int = 64) -> Any:
    """Main entry point: dispatches to the configured backend.

    settings.AI_BACKEND: 'local' (default), 'hf' (Hugging Face Inference), or 'openai'
    """
    backend = getattr(settings, 'AI_BACKEND', 'local')
    if backend == 'local':
        return summarize_via_local(prompt, max_length, min_length)
    elif backend == 'hf':
        return summarize_via_hf_inference(prompt, max_length, min_length)
    elif backend == 'openai':
        return summarize_via_openai(prompt, max_length, min_length)
    else:
        raise RuntimeError(f'Unknown AI_BACKEND: {backend}')


def is_model_loaded() -> bool:
    backend = getattr(settings, 'AI_BACKEND', 'local')
    if backend == 'local':
        return is_local_loaded()
    # For hosted backends we consider model "loaded" if credentials are set
    if backend == 'hf':
        return bool(getattr(settings, 'HF_API_TOKEN', None))
    if backend == 'openai':
        return bool(getattr(settings, 'OPENAI_API_KEY', None))
    return False


def get_model_name() -> Optional[str]:
    backend = getattr(settings, 'AI_BACKEND', 'local')
    if backend == 'local':
        return _local_model_name
    if backend == 'hf':
        return 'facebook/bart-large-cnn'
    if backend == 'openai':
        return getattr(settings, 'OPENAI_MODEL', 'openai')
    return None


def load_model_background():
    t = threading.Thread(target=load_local_model, daemon=True)
    t.start()
    return t
