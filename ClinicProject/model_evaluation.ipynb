{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ¯ AI Clinic - Waiting Time Prediction Model Evaluation\n",
    "## Machine Learning Model Performance Analysis\n",
    "\n",
    "**Model**: Random Forest Regressor  \n",
    "**Task**: Predict patient waiting time in minutes  \n",
    "**Features**: 7 input features (hour, day, queue position, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Django environment\n",
    "import os\n",
    "import sys\n",
    "import django\n",
    "\n",
    "sys.path.append('c:/Users/VITUS/AiClinicNew/ClinicProject')\n",
    "os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'clinic_token_system.settings')\n",
    "django.setup()\n",
    "\n",
    "print(\"âœ… Django environment loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "from api.waiting_time_predictor import waiting_time_predictor\n",
    "from api.models import Token\n",
    "from datetime import timedelta\n",
    "from django.utils import timezone\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"âœ… Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š Step 1: Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training data\n",
    "X, y = waiting_time_predictor.prepare_training_data(use_all_data=True)\n",
    "\n",
    "print(f\"ğŸ“Š Training Data Shape:\")\n",
    "print(f\"   Features (X): {X.shape}\")\n",
    "print(f\"   Target (y): {y.shape}\")\n",
    "print(f\"\\nğŸ“ˆ Sample Statistics:\")\n",
    "print(f\"   Mean waiting time: {np.mean(y):.2f} minutes\")\n",
    "print(f\"   Std deviation: {np.std(y):.2f} minutes\")\n",
    "print(f\"   Min: {np.min(y):.2f} minutes\")\n",
    "print(f\"   Max: {np.max(y):.2f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ Step 2: Train Model & Get Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train model\n",
    "print(\"ğŸ”„ Training model...\")\n",
    "success = waiting_time_predictor.train_model()\n",
    "\n",
    "if success:\n",
    "    print(\"âœ… Model trained successfully!\")\n",
    "    \n",
    "    # Load model and make predictions\n",
    "    waiting_time_predictor.load_model()\n",
    "    X_test_scaled = waiting_time_predictor.scaler.transform(X_test)\n",
    "    y_pred = waiting_time_predictor.model.predict(X_test_scaled)\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Predictions generated: {len(y_pred)} samples\")\n",
    "else:\n",
    "    print(\"âŒ Model training failed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ˆ Step 3: Regression Metrics (For Continuous Prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate regression metrics\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Calculate MAPE (Mean Absolute Percentage Error)\n",
    "mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸ¯ REGRESSION METRICS (Continuous Prediction)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nğŸ“Š Mean Absolute Error (MAE):     {mae:.2f} minutes\")\n",
    "print(f\"   â†’ On average, predictions are off by {mae:.2f} minutes\")\n",
    "print(f\"\\nğŸ“Š Root Mean Squared Error (RMSE): {rmse:.2f} minutes\")\n",
    "print(f\"   â†’ Penalizes larger errors more heavily\")\n",
    "print(f\"\\nğŸ“Š RÂ² Score (Coefficient of Determination): {r2:.4f}\")\n",
    "print(f\"   â†’ Model explains {r2*100:.2f}% of variance in waiting times\")\n",
    "print(f\"\\nğŸ“Š Mean Absolute Percentage Error (MAPE): {mape:.2f}%\")\n",
    "print(f\"   â†’ Average prediction error is {mape:.2f}% of actual value\")\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ Step 4: Classification Metrics (Accuracy & F1 Score)\n",
    "\n",
    "**Note**: Since this is a regression problem, we convert it to classification by binning predictions into categories:\n",
    "- **Short Wait**: 0-15 minutes\n",
    "- **Medium Wait**: 16-30 minutes\n",
    "- **Long Wait**: 31+ minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert continuous predictions to categories\n",
    "def categorize_wait_time(minutes):\n",
    "    if minutes <= 15:\n",
    "        return 'Short (0-15 min)'\n",
    "    elif minutes <= 30:\n",
    "        return 'Medium (16-30 min)'\n",
    "    else:\n",
    "        return 'Long (31+ min)'\n",
    "\n",
    "# Categorize actual and predicted values\n",
    "y_test_cat = [categorize_wait_time(y) for y in y_test]\n",
    "y_pred_cat = [categorize_wait_time(y) for y in y_pred]\n",
    "\n",
    "# Calculate classification metrics\n",
    "accuracy = accuracy_score(y_test_cat, y_pred_cat)\n",
    "f1 = f1_score(y_test_cat, y_pred_cat, average='weighted')\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸ¯ CLASSIFICATION METRICS (Categorical Prediction)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nâœ… Accuracy Score: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "print(f\"   â†’ Model correctly predicts wait time category {accuracy*100:.2f}% of the time\")\n",
    "print(f\"\\nâœ… F1 Score (Weighted): {f1:.4f}\")\n",
    "print(f\"   â†’ Harmonic mean of precision and recall: {f1*100:.2f}%\")\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š Step 5: Detailed Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate classification report\n",
    "print(\"\\nğŸ“‹ DETAILED CLASSIFICATION REPORT:\")\n",
    "print(\"=\"*60)\n",
    "print(classification_report(y_test_cat, y_pred_cat))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test_cat, y_pred_cat)\n",
    "print(\"\\nğŸ“Š CONFUSION MATRIX:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ˆ Step 6: Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Actual vs Predicted Scatter Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "plt.xlabel('Actual Waiting Time (minutes)', fontsize=12)\n",
    "plt.ylabel('Predicted Waiting Time (minutes)', fontsize=12)\n",
    "plt.title('Actual vs Predicted Waiting Times', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Scatter plot shows how close predictions are to actual values\")\n",
    "print(\"   Points closer to red line = better predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Prediction Error Distribution\n",
    "errors = y_test - y_pred\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(errors, bins=30, edgecolor='black', alpha=0.7)\n",
    "plt.axvline(x=0, color='r', linestyle='--', linewidth=2, label='Perfect Prediction')\n",
    "plt.xlabel('Prediction Error (minutes)', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.title('Distribution of Prediction Errors', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"âœ… Error distribution centered around {np.mean(errors):.2f} minutes\")\n",
    "print(f\"   Most predictions within Â±{np.std(errors):.2f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Confusion Matrix Heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Short', 'Medium', 'Long'],\n",
    "            yticklabels=['Short', 'Medium', 'Long'])\n",
    "plt.xlabel('Predicted Category', fontsize=12)\n",
    "plt.ylabel('Actual Category', fontsize=12)\n",
    "plt.title('Confusion Matrix - Wait Time Categories', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Diagonal values show correct predictions\")\n",
    "print(\"   Off-diagonal values show misclassifications\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Metrics Summary Bar Chart\n",
    "metrics = {\n",
    "    'Accuracy': accuracy * 100,\n",
    "    'F1 Score': f1 * 100,\n",
    "    'RÂ² Score': r2 * 100\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(metrics.keys(), metrics.values(), color=['#2ecc71', '#3498db', '#e74c3c'])\n",
    "plt.ylabel('Score (%)', fontsize=12)\n",
    "plt.title('Model Performance Metrics', fontsize=14, fontweight='bold')\n",
    "plt.ylim(0, 100)\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{height:.1f}%',\n",
    "             ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… All metrics above 80% indicate excellent model performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š Step 7: Final Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ† FINAL MODEL EVALUATION SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nğŸ“Š Dataset Information:\")\n",
    "print(f\"   Total Samples: {len(X)}\")\n",
    "print(f\"   Training Samples: {len(X_train)}\")\n",
    "print(f\"   Testing Samples: {len(X_test)}\")\n",
    "print(f\"   Features: {X.shape[1]}\")\n",
    "\n",
    "print(f\"\\nğŸ¯ Regression Metrics (Continuous Prediction):\")\n",
    "print(f\"   MAE:  {mae:.2f} minutes\")\n",
    "print(f\"   RMSE: {rmse:.2f} minutes\")\n",
    "print(f\"   RÂ²:   {r2:.4f} ({r2*100:.2f}%)\")\n",
    "print(f\"   MAPE: {mape:.2f}%\")\n",
    "\n",
    "print(f\"\\nğŸ¯ Classification Metrics (Categorical Prediction):\")\n",
    "print(f\"   Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "print(f\"   F1 Score: {f1:.4f} ({f1*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\nâœ… Model Performance Rating:\")\n",
    "if accuracy >= 0.85 and r2 >= 0.80:\n",
    "    print(\"   ğŸŒŸ EXCELLENT - Model is production-ready!\")\n",
    "elif accuracy >= 0.75 and r2 >= 0.70:\n",
    "    print(\"   âœ… GOOD - Model performs well\")\n",
    "elif accuracy >= 0.65 and r2 >= 0.60:\n",
    "    print(\"   âš ï¸  FAIR - Model needs improvement\")\n",
    "else:\n",
    "    print(\"   âŒ POOR - Model needs retraining\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âœ… Evaluation Complete!\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
